{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62966afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth\n",
    "\n",
    "Credit/author:Mattia Gatti\n",
    "https://towardsdatascience.com/generate-a-3d-mesh-from-an-image-with-python-12210c73e5cc\n",
    "pip install transformers\"\"\"\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import GLPNImageProcessor, GLPNForDepthEstimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a51303c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = GLPNImageProcessor.from_pretrained(\"vinvino02/glpn-nyu\")\n",
    "model = GLPNForDepthEstimation.from_pretrained(\"vinvino02/glpn-nyu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b21b25c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why The Sigmoid Function Is Important In Neural Networks?\\n\\nThe Sigmoid As A Squashing Function\\nThe sigmoid function is also called a squashing function as its domain is the set of all real numbers,\\nand its range is (0, 1). Hence, if the input to the function is either a very large negative number\\nor a very large positive number,  the output is always between 0 and 1.\\n\\nIf we use a linear activation function in a neural network, then this model can only learn linearly separable problems. \\nHowever, with the addition of just one hidden layer and a sigmoid activation function in the hidden layer, the neural\\nnetwork can easily learn a non-linearly separable problem. Using a non-linear function produces non-linear boundaries\\nand hence, the sigmoid function can be used in neural networks for learning complex decision functions.\\n\\nThe only non-linear function that can be used as an activation function in a neural network is one which is monotonically\\nincreasing. So for example, sin(x) or cos(x) cannot be used as activation functions.\\nAlso, the activation function should be defined everywhere and should be continuous everywhere in the space\\nof real numbers. The function is also required to be differentiable over the entire space of real numbers.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Why The Sigmoid Function Is Important In Neural Networks?\n",
    "\n",
    "The Sigmoid As A Squashing Function\n",
    "The sigmoid function is also called a squashing function as its domain is the set of all real numbers,\n",
    "and its range is (0, 1). Hence, if the input to the function is either a very large negative number\n",
    "or a very large positive number,  the output is always between 0 and 1.\n",
    "\n",
    "If we use a linear activation function in a neural network, then this model can only learn linearly separable problems. \n",
    "However, with the addition of just one hidden layer and a sigmoid activation function in the hidden layer, the neural\n",
    "network can easily learn a non-linearly separable problem. Using a non-linear function produces non-linear boundaries\n",
    "and hence, the sigmoid function can be used in neural networks for learning complex decision functions.\n",
    "\n",
    "The only non-linear function that can be used as an activation function in a neural network is one which is monotonically\n",
    "increasing. So for example, sin(x) or cos(x) cannot be used as activation functions.\n",
    "Also, the activation function should be defined everywhere and should be continuous everywhere in the space\n",
    "of real numbers. The function is also required to be differentiable over the entire space of real numbers.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d6b162f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"https://huggingface.co/docs/transformers/model_doc/glpn\n",
    "\n",
    "The monocular depth estimation model chosen for this guide is GLPN⁴. \n",
    "It is available on the Hugging Face Model Hub. Models can be retrieved from\n",
    "this hub by using the Hugging Face library Transformers.\"\"\"\n",
    "\n",
    "# load and resize the input image\n",
    "image = Image.open(\"C:/Users/seeho/assets/raw/m2.jpg\")\n",
    "new_height = 480 if image.height > 480 else image.height\n",
    "new_height -= (new_height % 32)\n",
    "new_width = int(new_height * image.width / image.height)\n",
    "diff = new_width % 32\n",
    "new_width = new_width - diff if diff < 16 else new_width + 32 - diff\n",
    "new_size = (new_width, new_height)\n",
    "image = image.resize(new_size)\n",
    "\n",
    "# prepare image for the model\n",
    "inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "# get the prediction from the model\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predicted_depth = outputs.predicted_depth\n",
    "\n",
    "# remove borders\n",
    "pad = 16\n",
    "output = predicted_depth.squeeze().cpu().numpy() * 1000.0\n",
    "output = output[pad:-pad, pad:-pad]\n",
    "image = image.crop((pad, pad, image.width - pad, image.height - pad))\n",
    "\n",
    "# visualize the prediction\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].imshow(image)\n",
    "ax[0].tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)\n",
    "ax[1].imshow(output, cmap='viridis')\n",
    "ax[1].tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)\n",
    "plt.tight_layout()\n",
    "plt.pause(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "843990b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' -Alpha shape¹, \\n    -Ball pivoting², \\n    -and Poisson surface reconstruction³\\n\\nThese methods are known as surface reconstruction algorithms.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" -Alpha shape¹, \n",
    "    -Ball pivoting², \n",
    "    -and Poisson surface reconstruction³\n",
    "\n",
    "These methods are known as surface reconstruction algorithms.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb33e635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Point cloud.\n",
    "   The following code converts the estimated depth map to an Open3D point cloud object.\n",
    "   http://www.open3d.org/docs/release/getting_started.html\"\"\"\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "\n",
    "width, height = image.size\n",
    "\n",
    "depth_image = (output * 255 / np.max(output)).astype('uint8')\n",
    "image = np.array(image)\n",
    "\n",
    "# create rgbd image\n",
    "depth_o3d = o3d.geometry.Image(depth_image)\n",
    "image_o3d = o3d.geometry.Image(image)\n",
    "rgbd_image = o3d.geometry.RGBDImage.create_from_color_and_depth(image_o3d, depth_o3d, convert_rgb_to_intensity=False)\n",
    "\n",
    "# camera settings\n",
    "camera_intrinsic = o3d.camera.PinholeCameraIntrinsic()\n",
    "camera_intrinsic.set_intrinsics(width, height, 500, 500, width/2, height/2)\n",
    "\n",
    "\"\"\"Create a point cloud for that merger of RGB + DI\"\"\"\n",
    "pcd = o3d.geometry.PointCloud.create_from_rgbd_image(rgbd_image, camera_intrinsic)\n",
    "\"\"\"Debugging for the point cloud distance\"\"\"\n",
    "pcd2 = pcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff183727",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"An RGBD Image is simply a combination of an RGB image and its corresponding depth image. \n",
    "PinholeCameraIntrinsic class stores what is known as the intrinsic camera matrix.\n",
    "Through this matrix, Open3D can create a point cloud from an RGBD image with the correct\n",
    "spacing between the points. Keep the intrinsic parameters as they are.\n",
    "\n",
    "To visualize the point cloud use below call\"\"\"\n",
    "o3d_pointer=o3d.visualization.draw_geometries([pcd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1606dbb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Don't run it if you intend to go to the mesh section below or kill it 1st.Need to unblock resorces 1st\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Don't run it if you intend to go to the mesh section below or kill it 1st.Need to unblock resorces 1st\"\"\"\n",
    "#o3d_pointer.destroy.window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8811f925",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Mesh Generator Unit.\n",
    "Among the various methods available in the literature for this task, the prudent approach would be so called \n",
    "Poisson surface reconstruction algorithm. This method has been chosen because it’s the one that usually gives better\n",
    "and smoother results.\n",
    "This code generates the mesh from the point cloud obtained in the last step above\n",
    "\n",
    "statistical_outlier_removal removes points that are further away from their neighbors compared to the average\n",
    "for the point cloud. \n",
    "It takes two input parameters:\n",
    "-nb_neighbors allows to specify how many neighbors are taken into account in order to calculate\n",
    "the average distance for a given point.\n",
    "\n",
    "-std_ratio allows to set the threshold level based on the standard deviation of the average distances\n",
    "across the point cloud. The lower this number the more aggressive the filter will be.\n",
    "\"\"\"\n",
    "# outliers removal\n",
    "cl, ind = pcd.remove_statistical_outlier(nb_neighbors=20, std_ratio=20.0)\n",
    "pcd = pcd.select_by_index(ind)\n",
    "\n",
    "# estimate normals\n",
    "pcd.estimate_normals()\n",
    "pcd.orient_normals_to_align_with_direction()\n",
    "\n",
    "\"\"\"Surface reconstruction\n",
    "Finally, the algorithm is executed. depth value defines the detail level of the mesh. \n",
    "A higher depth value aside from increasing mesh quality increases also the output dimensions.\"\"\"\n",
    "mesh = o3d.geometry.TriangleMesh.create_from_point_cloud_poisson(pcd, depth=10, n_threads=1)[0]\n",
    "\n",
    "# rotate the mesh\n",
    "rotation = mesh.get_rotation_matrix_from_xyz((np.pi, 0, 0))\n",
    "mesh.rotate(rotation, center=(0, 0, 0))\n",
    "\n",
    "\"\"\"Save the mesh\n",
    "Opening an OBJ file with a text editor\n",
    "OBJ file open in Microsoft Visual Studio Code\n",
    "OBJ file open in Microsoft Visual Studio Code\n",
    "Since OBJ files are saved in plain text, you can also open them with a text editor, such as Microsoft Notepad (Windows)\n",
    "or Apple TextEdit (Mac), or a source code editor. You may need to rename the .obj file extension to .txt\n",
    "for the text editor to recognize it.\n",
    "\n",
    "When opened in a text editor or source code editor, you can modify the properties of the OBJ file. \n",
    "Remember that if you incorrectly edit the file, you may inadvertently corrupt the model.\"\"\"\n",
    "\n",
    "o3d.io.write_triangle_mesh(f'./mesh.obj', mesh)\n",
    "\n",
    "# visualize the mesh\n",
    "o3d.visualization.draw_geometries([mesh], mesh_show_back_face=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaaec04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SHIFT Q isn't the correct way!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd944b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"DON'T EVENT THINK OF RUNNING IT HERE!!!\n",
    "\n",
    "\n",
    "For future research...Intel RealView cam pipeline\n",
    "\"\"\"\n",
    "pipeline1 = rs.pipeline()\n",
    "rs_config.enable_device(connect_device[0])\n",
    "pipeline_profile1 = pipeline1.start(rs_config)\n",
    "\n",
    "intr1 = pipeline_profile1.get_stream(rs.stream.color).as_video_stream_profile().get_intrinsics()\n",
    "pinhole_camera1_intrinsic = o3d.camera.PinholeCameraIntrinsic(intr1.width, intr1.height, intr1.fx, intr1.fy, intr1.ppx, intr1.ppy)\n",
    "cam1 = rgbdTools.Camera(intr1.fx, intr1.fy, intr1.ppx, intr1.ppy)\n",
    "# print('cam1 intrinsics:')\n",
    "# print(intr1.width, intr1.height, intr1.fx, intr1.fy, intr1.ppx, intr1.ppy)\n",
    "\n",
    "pipeline2 = rs.pipeline()\n",
    "rs_config.enable_device(connect_device[1])\n",
    "pipeline_profile2 = pipeline2.start(rs_config)\n",
    "\n",
    "intr2 = pipeline_profile2.get_stream(rs.stream.color).as_video_stream_profile().get_intrinsics()\n",
    "pinhole_camera2_intrinsic = o3d.camera.PinholeCameraIntrinsic(intr2.width, intr2.height, intr2.fx, intr2.fy, intr2.ppx, intr2.ppy)\n",
    "cam2 = rgbdTools.Camera(intr2.fx, intr2.fy, intr2.ppx, intr2.ppy)\n",
    "# print('cam2 intrinsics:')\n",
    "# print(intr2.width, intr2.height, intr2.fx, intr2.fy, intr2.ppx, intr2.ppy)\n",
    "\n",
    "print('Calculating Transformation Matrix:')\n",
    "cam1_point = []\n",
    "cam2_point = []\n",
    "for view in range(chessBoard_num):\n",
    "    cam1_rgb = cv2.imread('./output/cam1_color_'+str(view)+'.png')\n",
    "    cam1_rgb_array = np.asanyarray(cam1_rgb)\n",
    "    cam1_depth = cv2.imread('./output/cam1_depth_'+str(view)+'.png',-1)\n",
    "    cam1_depth_array = np.asanyarray(cam1_depth)\n",
    "    cam2_rgb = cv2.imread('./output/cam2_color_'+str(view)+'.png')\n",
    "    cam2_rgb_array = np.asanyarray(cam2_rgb)\n",
    "    cam2_depth = cv2.imread('./output/cam2_depth_'+str(view)+'.png',-1)\n",
    "    cam2_depth_array = np.asanyarray(cam2_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914880f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
